{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# **Parler TTS : TAMIL**"],"metadata":{"id":"bcZCNNprBieA"}},{"cell_type":"markdown","source":["## 1. Installation"],"metadata":{"id":"yi4GaJU_-zAx"}},{"cell_type":"markdown","source":["Cloning and Installing both Dataspeech and Parler-TTS repository from HuggingFace Hub"],"metadata":{"id":"UpUeRhMKBrWw"}},{"cell_type":"code","source":["!git clone https://github.com/huggingface/dataspeech.git\n","!cd dataspeech\n","! pip install --quiet -r ./dataspeech/requirements.txt"],"metadata":{"_uuid":"eb83b6a4-ad4f-4e2a-8ff5-b3f3dad1d616","_cell_guid":"5ebb6cde-8206-42c8-9282-708e9bdb9819","trusted":true,"id":"F2ypL-9LtE84","execution":{"iopub.status.busy":"2025-04-05T12:34:11.346392Z","iopub.execute_input":"2025-04-05T12:34:11.346667Z","iopub.status.idle":"2025-04-05T12:39:24.169351Z","shell.execute_reply.started":"2025-04-05T12:34:11.346642Z","shell.execute_reply":"2025-04-05T12:39:24.168318Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!git clone https://github.com/huggingface/parler-tts.git\n","%cd parler-tts\n","!pip install --quiet -e .[train]"],"metadata":{"_uuid":"8f3ddda5-79d1-4e67-bfa1-f6e389284564","_cell_guid":"b4dcb201-d952-4400-a257-7bc411129884","trusted":true,"id":"L7LI7DaMtLRX","execution":{"iopub.status.busy":"2025-04-05T12:39:24.171186Z","iopub.execute_input":"2025-04-05T12:39:24.171450Z","iopub.status.idle":"2025-04-05T12:40:07.113803Z","shell.execute_reply.started":"2025-04-05T12:39:24.171426Z","shell.execute_reply":"2025-04-05T12:40:07.112637Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install --upgrade protobuf wandb==0.16.6  # Updating the Protobuf library and Installing wandb (Weight & Biases)"],"metadata":{"_uuid":"42df1e0b-2f24-4ac2-a41b-39f723567e43","_cell_guid":"9033065e-e09d-4090-a011-75429419f837","trusted":true,"id":"_jOs3qtntNQn","execution":{"iopub.status.busy":"2025-04-05T12:42:59.544941Z","iopub.execute_input":"2025-04-05T12:42:59.545286Z","iopub.status.idle":"2025-04-05T12:43:04.596486Z","shell.execute_reply.started":"2025-04-05T12:42:59.545260Z","shell.execute_reply":"2025-04-05T12:43:04.595515Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install huggingface_hub  # Installing huggingface_hub library"],"metadata":{"_uuid":"9d7aad58-654b-4fec-809b-620aaddaa4e0","_cell_guid":"5df618cb-4e3c-49ce-8a6d-a6eb22e70864","trusted":true,"id":"LY5SHNFttO83","execution":{"iopub.status.busy":"2025-04-05T12:43:16.296992Z","iopub.execute_input":"2025-04-05T12:43:16.297333Z","iopub.status.idle":"2025-04-05T12:43:20.635134Z","shell.execute_reply.started":"2025-04-05T12:43:16.297306Z","shell.execute_reply":"2025-04-05T12:43:20.633897Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["Logging in to HuggingFace through the Authentication Token to Authenticate Operations like Pushing the Models to the hub or Loading the Datasets"],"metadata":{"id":"UTovD3XaDF9W"}},{"cell_type":"code","source":["!git config --global credential.helper store"],"metadata":{"_uuid":"1bed344d-2d2f-4613-855c-e5326717998a","_cell_guid":"44475252-349b-4fb3-98fd-181892fb48fe","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:43:26.428247Z","iopub.execute_input":"2025-04-05T12:43:26.428763Z","iopub.status.idle":"2025-04-05T12:43:26.549362Z","shell.execute_reply.started":"2025-04-05T12:43:26.428719Z","shell.execute_reply":"2025-04-05T12:43:26.548160Z"},"jupyter":{"outputs_hidden":false},"id":"j8DH19rf-Ll7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import login\n","login()"],"metadata":{"_uuid":"da369e2e-0849-46b4-b1e1-7a8a6d021cdb","_cell_guid":"6c227396-eedf-4b93-9ce3-707a22a9b52b","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:43:28.854683Z","iopub.execute_input":"2025-04-05T12:43:28.855073Z","iopub.status.idle":"2025-04-05T12:43:29.411680Z","shell.execute_reply.started":"2025-04-05T12:43:28.855044Z","shell.execute_reply":"2025-04-05T12:43:29.410726Z"},"jupyter":{"outputs_hidden":false},"id":"Utr16olH-Ll7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%cd ../dataspeech  # Setting the Directory to Dataspeech Folder"],"metadata":{"_uuid":"0f69a69c-b123-481b-ba99-b40f3b3bcf16","_cell_guid":"80476b4a-aba2-45da-9a1c-6da0601b49db","trusted":true,"id":"lWPYPuEgtRAX","execution":{"iopub.status.busy":"2025-04-05T12:43:54.942368Z","iopub.execute_input":"2025-04-05T12:43:54.942751Z","iopub.status.idle":"2025-04-05T12:43:54.948376Z","shell.execute_reply.started":"2025-04-05T12:43:54.942673Z","shell.execute_reply":"2025-04-05T12:43:54.947168Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 2. Loading Dataset"],"metadata":{"id":"CxTqquGX-8Ap"}},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"SPRINGLab/IndicTTS_Tamil\")"],"metadata":{"_uuid":"0b3b1d9e-18a1-4671-bc3b-b0a38621540f","_cell_guid":"76aa921a-0ed6-4d1d-acfd-52a24c7be681","trusted":true,"id":"_j-b2v8ItUIX","execution":{"iopub.status.busy":"2025-04-04T17:14:02.186273Z","iopub.execute_input":"2025-04-04T17:14:02.186602Z","iopub.status.idle":"2025-04-04T17:14:27.225674Z","shell.execute_reply.started":"2025-04-04T17:14:02.186576Z","shell.execute_reply":"2025-04-04T17:14:27.224983Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from IPython.display import Audio\n","print(dataset[\"train\"][0][\"transcription\"])\n","Audio(dataset[\"train\"][0][\"audio\"][\"array\"], rate=dataset[\"train\"][0][\"audio\"][\"sampling_rate\"])"],"metadata":{"_uuid":"250a897b-74c8-4f95-b174-b8880ea71e11","_cell_guid":"480c2a2b-a923-4d44-9f37-7f61e7beeabc","trusted":true,"id":"uQGsJaNGtcPP","execution":{"iopub.status.busy":"2025-04-04T17:15:21.927498Z","iopub.execute_input":"2025-04-04T17:15:21.927871Z","iopub.status.idle":"2025-04-04T17:15:21.970019Z","shell.execute_reply.started":"2025-04-04T17:15:21.927842Z","shell.execute_reply":"2025-04-04T17:15:21.969076Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from IPython.display import Audio\n","print(dataset[\"train\"][1][\"transcription\"])\n","Audio(dataset[\"train\"][1][\"audio\"][\"array\"], rate=dataset[\"train\"][1][\"audio\"][\"sampling_rate\"])"],"metadata":{"_uuid":"f7cea629-690f-4eca-90aa-d8345d845d8d","_cell_guid":"88fdb7ce-0aec-4edb-97ab-cf2e4f569bb9","trusted":true,"id":"9_8Tf_2bte_3","execution":{"iopub.status.busy":"2025-04-04T17:15:38.579965Z","iopub.execute_input":"2025-04-04T17:15:38.580259Z","iopub.status.idle":"2025-04-04T17:15:38.591605Z","shell.execute_reply.started":"2025-04-04T17:15:38.580236Z","shell.execute_reply":"2025-04-04T17:15:38.590735Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["del dataset"],"metadata":{"_uuid":"af183fa4-3e7e-4783-95d7-9bac03b09936","_cell_guid":"c949da7b-a20f-4441-8d89-ad921e196e11","trusted":true,"id":"mFINUtC6tgy3","execution":{"iopub.status.busy":"2025-04-04T17:16:27.764910Z","iopub.execute_input":"2025-04-04T17:16:27.765270Z","iopub.status.idle":"2025-04-04T17:16:27.769672Z","shell.execute_reply.started":"2025-04-04T17:16:27.765242Z","shell.execute_reply":"2025-04-04T17:16:27.768713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 3. Annotating the Dataset"],"metadata":{"id":"V_07c9lI_Yig"}},{"cell_type":"markdown","source":["Through the \"main.py\" script, The Dataset receives metadata annotations which include pitch, SNR, reverberation and speech rate parameters. The dataset will gain important acoustic attributes through these parameters."],"metadata":{"id":"i4kF_zu5Eil3"}},{"cell_type":"code","source":["!python main.py \"SPRINGLab/IndicTTS_Tamil\" \\\n","  --configuration \"default\" \\\n","  --text_column_name \"text\" \\\n","  --audio_column_name \"audio\" \\\n","  --cpu_num_workers 1 \\\n","  --num_workers_per_gpu_for_pitch 1 \\\n","  --rename_column \\\n","  --repo_id \"IndicTTS-Tamil-tags\""],"metadata":{"_uuid":"26412f57-fbdd-44be-bed5-40c8b121007a","_cell_guid":"8d31da2f-90bf-483a-bc4a-2c53a693da6a","trusted":true,"id":"3GlGSmlqtiFf","execution":{"iopub.status.busy":"2025-04-04T17:16:57.495841Z","iopub.execute_input":"2025-04-04T17:16:57.496157Z","iopub.status.idle":"2025-04-04T17:48:20.860955Z","shell.execute_reply.started":"2025-04-04T17:16:57.496131Z","shell.execute_reply":"2025-04-04T17:48:20.859807Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Checking out the Annotated Dataset\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"SrihariGKS/IndicTTS-Tamil-tags\")\n","print(\"SNR 1st sample\", dataset[\"train\"][0][\"snr\"])\n","print(\"C50 2nd sample\", dataset[\"train\"][0][\"c50\"])\n","del dataset"],"metadata":{"_uuid":"46fbbbd9-8543-4fa7-b1d6-bbf359c2fc27","_cell_guid":"b1ad2f09-b1be-43b1-9533-d5c2ebbe4092","trusted":true,"id":"pGVnn5SLtwaP","execution":{"iopub.status.busy":"2025-04-04T17:52:01.787293Z","iopub.execute_input":"2025-04-04T17:52:01.787596Z","iopub.status.idle":"2025-04-04T17:52:04.172149Z","shell.execute_reply.started":"2025-04-04T17:52:01.787572Z","shell.execute_reply":"2025-04-04T17:52:04.171305Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 4. Metadata to text bins"],"metadata":{"id":"jrVnQFq8_1oZ"}},{"cell_type":"markdown","source":[" The metadata-enriched dataset gets converted into standardized text bins which label metadata attributes using textual values (bins). This is done to simplify or standardize features by transforming them into text-based divisions for easier categorization."],"metadata":{"id":"7YxH5Ln9GERW"}},{"cell_type":"code","source":["!python ./scripts/metadata_to_text.py \\\n","    \"SrihariGKS/IndicTTS-Tamil-tags\" \\\n","    --repo_id \"IndicTTS-Tamil-tags\" \\\n","    --configuration \"default\" \\\n","    --cpu_num_workers 2 \\\n","    --path_to_bin_edges \"./examples/tags_to_annotations/v01_bin_edges.json\" \\\n","    --avoid_pitch_computation"],"metadata":{"_uuid":"82f95d34-06b5-4ed5-85d5-cf98bc36dbee","_cell_guid":"b81d8cf1-68f8-4dc5-b478-21a32c1620ac","trusted":true,"id":"ZcuU3IFFt0fw","execution":{"iopub.status.busy":"2025-04-04T17:52:54.507821Z","iopub.execute_input":"2025-04-04T17:52:54.508190Z","iopub.status.idle":"2025-04-04T17:53:09.731789Z","shell.execute_reply.started":"2025-04-04T17:52:54.508157Z","shell.execute_reply":"2025-04-04T17:53:09.730676Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Checking out whether the metadata has been succesfully categorized\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"SrihariGKS/IndicTTS-Tamil-tags\")\n","print(\"Noise 1st sample:\", dataset[\"train\"][0][\"noise\"])\n","print(\"Speaking rate 2nd sample:\", dataset[\"train\"][0][\"speaking_rate\"])\n","del dataset"],"metadata":{"_uuid":"aea714b3-397e-400c-a4d5-384d1dfe6d19","_cell_guid":"4439d164-f18e-41d9-8a78-532797fb961a","trusted":true,"id":"kRKc2QdluAQf","execution":{"iopub.status.busy":"2025-04-04T17:53:21.863088Z","iopub.execute_input":"2025-04-04T17:53:21.863412Z","iopub.status.idle":"2025-04-04T17:53:32.328230Z","shell.execute_reply.started":"2025-04-04T17:53:21.863378Z","shell.execute_reply":"2025-04-04T17:53:32.327559Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 5. Creating a Dataset for Speaker Descriptions"],"metadata":{"id":"KxbPlQ5x_-h4"}},{"cell_type":"markdown","source":["The \"run_prompt_creation\" script produces textual description prompts from dataset metadata for the Speaker. The descriptions contain natural language contexts including speaking style and quality from both model training and inference sessions."],"metadata":{"id":"zKLVyv4qMwJ8"}},{"cell_type":"code","source":["!python ./scripts/run_prompt_creation.py \\\n","  --speaker_name \"Ananya\" \\\n","  --is_single_speaker \\\n","  --dataset_name \"SrihariGKS/IndicTTS-Tamil-tags\" \\\n","  --output_dir \"./tmp_Ananya\" \\\n","  --dataset_config_name \"default\" \\\n","  --model_name_or_path \"google/gemma-2-2b-it\" \\\n","  --per_device_eval_batch_size 12 \\\n","  --attn_implementation \"sdpa\" \\\n","  --dataloader_num_workers 2 \\\n","  --push_to_hub \\\n","  --hub_dataset_id \"IndicTTS-Tamil-tagged\" \\\n","  --preprocessing_num_workers 2"],"metadata":{"_uuid":"9eaaaddc-89b5-443f-957a-e9194f7f2913","_cell_guid":"fb0c711f-07f8-49dc-a4db-05e07821b447","trusted":true,"id":"3XuchjLKuCin","execution":{"iopub.status.busy":"2025-04-04T17:54:03.604018Z","iopub.execute_input":"2025-04-04T17:54:03.604328Z","iopub.status.idle":"2025-04-04T19:23:52.417065Z","shell.execute_reply.started":"2025-04-04T17:54:03.604303Z","shell.execute_reply":"2025-04-04T19:23:52.415977Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Checking out the Prompt Dataset\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"SrihariGKS/IndicTTS-Tamil-tagged\")\n","print(\"1st sample:\", dataset[\"train\"][0][\"text_description\"])\n","print(\"2nd sample:\", dataset[\"train\"][1][\"text_description\"])\n","del dataset"],"metadata":{"_uuid":"8d3c33de-c7d3-4d8d-bab8-56574f30cdf9","_cell_guid":"b7e05a84-1f19-42d0-a3c8-8e16eb518e6c","trusted":true,"id":"FBoLixzhuWgH","execution":{"iopub.status.busy":"2025-04-04T19:27:04.024281Z","iopub.execute_input":"2025-04-04T19:27:04.024667Z","iopub.status.idle":"2025-04-04T19:27:18.949343Z","shell.execute_reply.started":"2025-04-04T19:27:04.024623Z","shell.execute_reply":"2025-04-04T19:27:18.948409Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 6. Fine-Tuning the Model"],"metadata":{"id":"IRMeBSjJALUK"}},{"cell_type":"code","source":["%cd ../parler-tts # Setting the Directory to Dataspeech Folder"],"metadata":{"_uuid":"4b385fae-6992-4eab-bb97-39286fdd21fa","_cell_guid":"4a011d0a-69e6-4f25-a3e1-f784e2e30325","trusted":true,"id":"eKMyNnu5uYbv","execution":{"iopub.status.busy":"2025-04-05T12:44:07.148767Z","iopub.execute_input":"2025-04-05T12:44:07.149094Z","iopub.status.idle":"2025-04-05T12:44:07.154849Z","shell.execute_reply.started":"2025-04-05T12:44:07.149058Z","shell.execute_reply":"2025-04-05T12:44:07.154015Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["The \"run_parler_tts_training\" script uses specified training and evaluation datasets for fine-tuning Parler TTS through executed commands. Usage of GPU acceleration through \"accelerate\" allows efficient training while letting the dataset define it's names and evaluation metrics alongside batch sizes."],"metadata":{"id":"6-UEyW9fP7Oy"}},{"cell_type":"code","source":["!accelerate launch ./training/run_parler_tts_training.py \\\n","    --model_name_or_path \"ai4bharat/indic-parler-tts\" \\                           # Pre-trained model to fine-tune\n","    --feature_extractor_name \"ylacombe/dac_44khz\" \\                               # Feature extractor for audio processing\n","    --description_tokenizer_name \"ai4bharat/indic-parler-tts\" \\                   # Tokenizer for descriptive text metadata\n","    --prompt_tokenizer_name \"ai4bharat/indic-parler-tts\" \\                        # Tokenizer for text prompts\n","    --report_to \"wandb\" \\                                                         # Logs training metrics and progress to Weights & Biases (wandb)\n","    --overwrite_output_dir true \\                                                 # Overwrites the output directory if it exists\n","    --train_dataset_name \"SPRINGLab/IndicTTS_Tamil\" \\                             # Training dataset containing audio and text\n","    --train_metadata_dataset_name \"SrihariGKS/IndicTTS-Tamil-tagged\" \\            # Additional training metadata (descriptions, tags).\n","    --train_dataset_config_name \"default\" \\                                       # Configuration name for the training dataset\n","    --train_split_name \"train\" \\                                                  # Split of the training dataset to use\n","    --eval_dataset_name \"SPRINGLab/IndicTTS_Tamil\" \\                              # Evaluation dataset for the Model Performance\n","    --eval_metadata_dataset_name \"SrihariGKS/IndicTTS-Tamil-tagged\" \\             # Evaluation metadata dataset\n","    --eval_dataset_config_name \"default\" \\                                        # Dataset configuration for evaluation\n","    --eval_split_name \"train\" \\                                                   # Dataset split used for evaluation\n","    --max_eval_samples 2 \\                                                        # Limits evaluation to first 8 samples for faster periodic evaluation\n","    --per_device_eval_batch_size 2 \\                                              # Number of samples per evaluation batch on each GPU/device\n","    --target_audio_column_name \"audio\" \\                                          # Column name containing audio data\n","    --description_column_name \"text_description\" \\                                # Column name containing textual descriptions\n","    --prompt_column_name \"text\" \\                                                 # Column name containing text prompts\n","    --max_duration_in_seconds 20 \\                                                # Maximum duration of audio samples in seconds\n","    --min_duration_in_seconds 2.0 \\                                               # Minimum duration of audio samples in seconds\n","    --max_text_length 400 \\                                                       # Maximum length of text prompts in tokens\n","    --preprocessing_num_workers 2 \\                                               # Number of CPU workers for preprocessing\n","    --do_train true \\                                                             # Enables training\n","    --num_train_epochs 10 \\                                                       # Number of training epochs\n","    --gradient_accumulation_steps 18 \\                                            # Accumulates gradients over multiple batches before updating model weights\n","    --gradient_checkpointing true \\                                               # Reducing GPU memory usage by checkpointing gradients, enabling larger models or batches\n","    --per_device_train_batch_size 2 \\                                             # Number of samples per batch per device for training\n","    --learning_rate 0.00003 \\                                                     # Initial learning rate used by the Adam optimizer\n","    --adam_beta1 0.9 \\                                                            # Beta1 hyperparameter for Adam optimizer\n","    --adam_beta2 0.99 \\                                                           # Beta2 hyperparameter for Adam optimizer\n","    --weight_decay 0.01 \\                                                         # Weight decay coefficient for regularization, preventing overfitting\n","    --max_grad_norm 0.8 \\                                                         # Applies gradient clipping by limiting gradient norm to 0.8, preventing gradient explosion and enhancing training stability.\n","    --lr_scheduler_type \"constant_with_warmup\" \\                                  # Learning rate schedule type: keeps learning rate constant after initial warm-up\n","    --warmup_steps 50 \\                                                           # Number of warmup steps for the learning rate scheduler\n","    --logging_steps 500 \\                                                         # Logs training metrics every 500 steps\n","    --save_strategy \"steps\" \\                                                     # Model checkpoint saving strategy based on step intervals\n","    --save_steps 1000 \\                                                           # Saves model checkpoints every 1000 steps\n","    --save_total_limit 1 \\                                                        # Limits the total number of saved checkpoints to 1\n","    --freeze_text_encoder true \\                                                  # Freezes weights of the text encoder, preventing updates during training to retain original pretrained representations\n","    --audio_encoder_per_device_batch_size 2 \\                                     # Batch size for audio encoder processing on each device\n","    --dtype \"float16\" \\                                                           # Data type precision used for computations (float16 for reduced memory usage and faster computation)\n","    --seed 456 \\                                                                  # Random seed for reproducibility of training\n","    --output_dir \"./output_dir_training/\" \\                                       # Directory to save trained model checkpoints and outputs\n","    --temporary_save_to_disk \"./audio_code_tmp/\" \\                                # Temporary directory for intermediate audio codes or data storage\n","    #--resume_from_checkpoint \"./output_dir_training/checkpoint-1500-epoch-1\" \\   # Resumes the Model Training from the saved Checkpoint in case if the model stops training abruptly\n","    --save_to_disk \"./tmp_dataset_audio/\" \\                                       # Directory to save processed audio datasets to disk\n","    --dataloader_num_workers 2 \\                                                  # Number of subprocesses to use for loading data\n","    --do_eval \\                                                                   # Enables evaluation during training\n","    --predict_with_generate \\                                                     # Uses generative approach during evaluation (model generates output to compute metrics)\n","    --include_inputs_for_metrics \\                                                # Includes inputs when computing evaluation metrics, essential for detailed metric logging\n","    --group_by_length true                                                        # Groups audio samples by similar length to optimize batch processing efficiency.\n"],"metadata":{"_uuid":"441a0a27-e6b5-478c-953c-78cfd791e58a","_cell_guid":"b1b1d4e7-b32f-4b23-8522-5427715a19dc","trusted":true,"id":"7P9rgSBluau4","execution":{"iopub.status.busy":"2025-04-05T12:48:00.228118Z","iopub.execute_input":"2025-04-05T12:48:00.228445Z","iopub.status.idle":"2025-04-05T18:49:15.782918Z","shell.execute_reply.started":"2025-04-05T12:48:00.228420Z","shell.execute_reply":"2025-04-05T18:49:15.781799Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 7. Model Inference"],"metadata":{"id":"wwFtPJyQAUqg"}},{"cell_type":"markdown","source":["Testing out the Model  by generating the speech samples through different prompts and Descriptions"],"metadata":{"id":"Fr9RDorYQcaC"}},{"cell_type":"code","source":["# Loading the Model and the Tokenizer\n","\n","from parler_tts import ParlerTTSForConditionalGeneration\n","from transformers import AutoTokenizer\n","import torch\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","tam_model = ParlerTTSForConditionalGeneration.from_pretrained(\"./output_dir_training\", torch_dtype=torch.float16).to(device)\n","tam_tokenizer = AutoTokenizer.from_pretrained(\"./output_dir_training\")\n","tam_description_tokenizer = AutoTokenizer.from_pretrained(tam_model.config.text_encoder._name_or_path)"],"metadata":{"_uuid":"4e1ab27d-32f2-40fd-8c3e-c2448922c837","_cell_guid":"e64a425c-231a-4324-bb1b-72e839eb2bbb","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:50:46.897509Z","iopub.execute_input":"2025-04-05T18:50:46.897938Z","iopub.status.idle":"2025-04-05T18:51:30.851949Z","shell.execute_reply.started":"2025-04-05T18:50:46.897908Z","shell.execute_reply":"2025-04-05T18:51:30.850874Z"},"jupyter":{"outputs_hidden":false},"id":"keBgKba6-Ll9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["tam_prompt = \"இந்த இடத்தை நான் எங்கே காணலாம்? ஒரு மணி நேரமாக தேடிக் கொண்டிருக்கிறேன்\"\n","tam_description = \"A Female Speaker's high-pitched, engaging voice is captured in a clear, close-sounding recording. Her slightly slower delivery conveys a positive tone.\"\n","\n","tam_description_input_ids = tam_description_tokenizer(tam_description, return_tensors=\"pt\").to(device)\n","tam_prompt_input_ids = tam_tokenizer(tam_prompt, return_tensors=\"pt\").to(device)\n","\n","tam_generation = tam_model.generate(input_ids=tam_description_input_ids.input_ids, attention_mask=tam_description_input_ids.attention_mask, prompt_input_ids=tam_prompt_input_ids.input_ids, prompt_attention_mask=tam_prompt_input_ids.attention_mask)\n","tam_audio_arr = tam_generation.cpu().numpy().squeeze()\n","tam_audio_arr = tam_audio_arr.astype(\"float32\")\n","\n","from IPython.display import Audio\n","Audio(tam_audio_arr, rate=tam_model.config.sampling_rate)"],"metadata":{"_uuid":"8d0d2fbf-1df4-40bf-a3d1-2e479bdbb1f6","_cell_guid":"2adc1fd9-e8e4-41d5-979d-1bc5cc5b78b1","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:03:43.164891Z","iopub.execute_input":"2025-04-05T19:03:43.165248Z","iopub.status.idle":"2025-04-05T19:03:49.767666Z","shell.execute_reply.started":"2025-04-05T19:03:43.165222Z","shell.execute_reply":"2025-04-05T19:03:49.766640Z"},"jupyter":{"outputs_hidden":false},"id":"XSyG_hYy-Ll-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 8. Pushing the Model to Huggingface Hub"],"metadata":{"id":"K0csvs-pAgXw"}},{"cell_type":"code","source":["# Pushes the fine-tuned model to the Hugging Face Hub\n","\n","model.push_to_hub(\"parler-tts-fine-tuned-tamil\")\n","tokenizer.push_to_hub(\"parler-tts-fine-tuned-tamil\")"],"metadata":{"_uuid":"b1056980-1126-44a4-9f77-0328450819ac","_cell_guid":"328ebc26-f48d-4dc1-94a6-460bfe0c635d","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:51:46.795676Z","iopub.execute_input":"2025-04-05T18:51:46.796075Z","iopub.status.idle":"2025-04-05T18:52:26.539889Z","shell.execute_reply.started":"2025-04-05T18:51:46.796046Z","shell.execute_reply":"2025-04-05T18:52:26.538984Z"},"jupyter":{"outputs_hidden":false},"id":"ANdPss-V-Ll9"},"outputs":[],"execution_count":null}]}